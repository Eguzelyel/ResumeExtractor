{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Keywords\n",
    "\n",
    "This is another place for finding the way to best efficiently extracting keywords.\n",
    "\n",
    "The end result of this notebook is implemented under keyword_extraction.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of pages: 1\n",
      "ekrem guzelyel b.s. m.s in computer science 2020 ekremguzelyel@gmail.com linkedin.com in ekrem guzelyel 224 830 1998 chicago il 60642 education illinois institute of technology chicago expected dec ember 2020 co terminal degree in computer science b.s. m.s. joint degree specialization in computational intelligence minor in applied mathematics courses taken elem. linear algebra discrete structures data structures algorithms data mining probability statistics machine learning deep learning database organizations gpa 3. 47 4.00 work experience projects machine learning lab at iit undergraduate research assistant chicago il october 2018 present research data categorization for transparent text classification summarize preprocess movie reviews to desired format. train a convolutional neural network cnn model to classify text document s. automate b ack propagat ion to identify reasoning for the decisions using multiple models. google engineering practicum intern ep mountain view ca may 2018 august 2018 project 1 improving targeting quality using tensorflow model implemented a negative filtering for google search ads using deep neural networks . created automated experiments to t une tensorflow model performance. observed significant improvements of quality 7.2 over traditional logistic regression model. project 2 model age monitor created tested and launched an online monitoring system that successfully catches old models and executes on call alert using python. arc supplemental instructor si august 2017 november 2017 held weekly tutoring and review sessions for calculus i research projects chicago pothole problem pothole filler route january 2019 present solve traveling salesman problem for the most efficient way to fill potholes looking at historical data. predicting disease span using telomere lengths sparta hack v january 2019 cleaned and preprocessed n ih survey da ta and trained a dnn to predict telomere lengths. locating weaponry through security cameras iit vlsi lab september 2017 november 2017 collected and labeled video and photo data using labelimg vatic and imagenet analyzing the effects of public housing toki to elections rice university august 2015 september 2015 organized and analyzed data for gender and geographical distribution of votes using java skills technical python numpy pandas sklearn tensorflow keras jupyter notebook java oracle sql languages english fluent turkish fluent spanish starter awards and honors dean s list electrical engineering departme nt iit fall 2016 spring 2017 national university entrance exam 1 st 2 nd stage ygs lys march june 2014 ranked in top 25 among 2.086.087 students physics olympiad student 2010 2012 extra curricular activities ml@iit machine learning club at iit events chair november 2018 present weekend school teacher and mentor at tasc september 2016 present teach turkish classes volunteer for 1 5th grades organize and lead summer and winter camps for high schoolers university prep presentations and how to be successful seminars march 2015 16 seminars given in istanbul kocaeli sakarya turkey recent hackathons attended boilermake lyfthack 2 nd place spartahack v\n"
     ]
    }
   ],
   "source": [
    "import re # For regex\n",
    "import PyPDF2 # For pdfs\n",
    "import random # For sampling\n",
    "import glob # For multiple files\n",
    "import keyword_extraction\n",
    "\n",
    "\n",
    "# Opening PDF file and cleaning text.\n",
    "\n",
    "pdfFileObj = open('./SamplePDF/Sample_Resume.pdf', 'rb') \n",
    "pdfReader = PyPDF2.PdfFileReader(pdfFileObj) \n",
    "\n",
    "print(\"# of pages:\", pdfReader.numPages) \n",
    " \n",
    "pageObj = pdfReader.getPage(0) \n",
    "text = pageObj.extractText()\n",
    "pdfFileObj.close() \n",
    "\n",
    "# Clean spaces and handle special characters.\n",
    "text = re.sub(\"\\s\\s+\", \" \", text).lower()\n",
    "text = text.replace(\"ó\",\"\\\"\").replace(\"ò\",\"\\\"\").replace(\"¥!\", \"\").replace(\"õ\",\"'\").replace(\"\\n\",\" \")\n",
    "\n",
    "# Apply regex to get list of words.\n",
    "reg_list = (re.split(r'[^(\\w+(\\.+|@)\\w+)]|[()]', text))\n",
    "\n",
    "# Filter None values. -> Regex could be implemented better to eliminate this step. \n",
    "# but, it works. So don't touch it :D\n",
    "key_list = list(filter(None,reg_list))\n",
    "final_text = \" \".join(key_list)\n",
    "\n",
    "# print(text)\n",
    "print(final_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper words, keywords\n",
    "Hide these in another folder as text files later on.\n",
    "- See below for easier way to implement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Action Words\n",
    "# Taken from https://www.thebalancecareers.com/list-of-resume-and-cover-letter-keywords-2060287\n",
    "\n",
    "fa = open(r\"./keywords/action_key.txt\", \"r\")\n",
    "action_words = fa.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_set = set(re.split(', |\\n+', action_words.lower())) # another way is '\\W+'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "performed\n",
      "multiplied\n",
      "judged\n",
      "experienced\n",
      "welcomed\n",
      "assembled\n",
      "drafted\n",
      "built\n",
      "updated\n",
      "measured\n"
     ]
    }
   ],
   "source": [
    "# Sample from action_set\n",
    "for num, word in enumerate(random.sample(action_set, 10)): print(word)\n",
    "# [print(word) for num, word in enumerate(random.sample(action_set,10))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_set = set(key_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(332, 267)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_set), len(action_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_set&action_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'analyzed',\n",
       " 'created',\n",
       " 'expected',\n",
       " 'implemented',\n",
       " 'launched',\n",
       " 'observed',\n",
       " 'organized',\n",
       " 'ranked',\n",
       " 'tested',\n",
       " 'trained'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_set.intersection(action_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open keywords for \"Computer Skills\"\n",
    "f = open(r\"./keywords/comp_sci_key.txt\", \"r\")\n",
    "computer_skills = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['based',\n",
       " 'analyze',\n",
       " 'testing',\n",
       " 'reporting',\n",
       " 'aros',\n",
       " 'analytics',\n",
       " 'logical',\n",
       " 'optimizing',\n",
       " 'machine',\n",
       " 'third']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comp_skill_set = set(re.split('\\W+', computer_skills.lower()))\n",
    "random.sample(comp_skill_set,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Soft Skills\n",
    "fs = open(r\"./keywords/soft_skills.txt\", \"r\")\n",
    "soft_skills = fs.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['energy',\n",
       " 'relationships',\n",
       " 'communication',\n",
       " 'monitoring',\n",
       " 'etiquette',\n",
       " 'influential',\n",
       " 'managing',\n",
       " 'conversations',\n",
       " 'storytelling',\n",
       " 'adaptable']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soft_skill_set = set(re.split('\\W+', soft_skills.lower()))\n",
    "random.sample(soft_skill_set,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Even an easier way__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_dic = {}\n",
    "stop_words = ['with','to', 'of','the', 'a', 'and','on']\n",
    "\n",
    "list_of_files = glob.glob('./keywords/*.txt')           # create the list of file\n",
    "for file_name in list_of_files:\n",
    "#     print(file_name[11:-4])\n",
    "\n",
    "    FI = open(file_name, 'r')\n",
    "    keys = FI.read()\n",
    "    \n",
    "    keyword_dic[file_name[11:-4]] = set(re.split('\\W+', keys.lower())).difference(stop_words)\n",
    "\n",
    "\n",
    "    FI.close()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# stop_words = ['with','to', 'of','the', 'a', 'and','on']\n",
    "keyword_dic['action_key']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_key ['set', 'utilized', 'illustrated', 'headed', 'invested']\n",
      "hard_skills ['legal', 'manufacturing', 'engineering', 'carpentry', 'bookkeeping']\n",
      "soft_skills ['social', 'independent', 'deal', 'writing', 'solving']\n",
      "comp_sci_key ['technologies', 'languages', 'coding', 'aros', 'interfaces']\n",
      "legal_key ['real', 'court', 'litigation', 'criminology', 'notarization']\n"
     ]
    }
   ],
   "source": [
    "for i in keyword_dic:\n",
    "    print(i, random.sample(keyword_dic[i],5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keywords imported.\n",
      "dict_keys(['action_key', 'hard_skills', 'soft_skills', 'comp_sci_key', 'legal_key'])\n",
      "Resume found.\n",
      "# of pages: 1\n",
      "Resume successfully extracted\n",
      "ekrem guzelyel \n"
     ]
    }
   ],
   "source": [
    "## Trying new keyword_extraction.py\n",
    "new_dic = keyword_extraction.import_keys()\n",
    "print(new_dic.keys())\n",
    "\n",
    "new_resume_keys, new_resume_text = keyword_extraction.import_resume()\n",
    "print(new_resume_text[:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing resume with keywords\n",
    "\n",
    "-> _This is left for another notebook_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keyword references:\n",
    "\n",
    "Action Words, hard, soft skills, CS - https://www.thebalancecareers.com/list-of-resume-and-cover-letter-keywords-2060287\n",
    "\n",
    "Legal - https://aneliteresume.com/resume-writing/keywords-are-key-law/\n",
    "\n",
    "------\n",
    "## The Idea\n",
    "\n",
    "Create a panel for percentage of how good a candidate is. \n",
    "- Find average of keywords occuring in a resume.\n",
    "    - i.e. 15 action_keywords is a wonderful sample. Show it as green pie chart.\n",
    "- Extract email, education, major\n",
    "- Find matches with skills.\n",
    "    - Return buzz words like Research, Machine Learning, Deep Neural Networks, Marketing, Java...\n",
    "- Find companies that match.\n",
    "    - Machine Learning can be implemented in this part. \n",
    "    - Label marketing, negotiation, sales as 1, coding, python as 0; then try to predict which category the applicant belongs.\n",
    "- Give volunteer score.\n",
    "    - Again, keywords like helped, organized, free should do.\n",
    "- Find extracurricular score.\n",
    "    - Brainstorm. A keyword list for extracurricular actions might work.\n",
    "    \n",
    "After evaluations, based on the score of the candidate make recommendations (both for recruiters and candidates). \n",
    "- Eg. Your extracurricular seems insufficient. Do something! (for candidate)\n",
    "- This applicant is a good match for \"Software Engineering Internship\" position. (for recruiters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
